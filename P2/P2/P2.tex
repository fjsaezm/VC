\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi
    
    \usepackage[sfdefault,scaled=.85, lining]{FiraSans}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{PrÃ¡ctica 2\\ Convolutional Neural Networks}
    \author{Javier SÃ¡ez Maldonado}

   
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    \maketitle
    \newpage
    
    \tableofcontents
    
    \newpage
    
    \section*{IntroducciÃ³n}
    
    Durante el prÃ³ximo documento, abordaremos el problema de construir una \textbf{Red neuronal convolucional}, y tratar de estudiar cÃ³mo, dado, un modelo, modificarlo para de diversas formas para tratar de obtener unos buenos resultados en el problema de clasificaciÃ³n de imÃ¡genes.
    
   	
    

    
    \hypertarget{primeras-funciones}{%
\section{Primeras
funciones}\label{primeras-funciones}}

   

    \hypertarget{funciones-de-generaciuxf3n-de-datos}{%
\subsection{Funciones de generaciÃ³n de
datos}\label{funciones-de-generaciuxf3n-de-datos}}

Vamos a crear ahora varias funciones que harÃ¡n ``lo mismo'', que es
devolver un objeto de tipo \emph{ImageDataGenerator}. ComentÃ©moslas antes de aÃ±adir todo el cÃ³digo de las mismas. Lo aÃ±adismos pues considero que serÃ¡ relevante para el resto de la prÃ¡ctica, pues se usarÃ¡n en muchas ocasiones.

\begin{itemize}
\item La primera es simple$\_$datagen, que harÃ¡ simplemente un objeto de la clase que haga una particiÃ³n de validaciÃ³n del $10\%$ tamaÃ±o total

\item La segunda ,normalize$\_$datagen, ademÃ¡s de aÃ±adir la validaciÃ³n, harÃ¡ que las imÃ¡genes tengan $\mu = 0$ y $\sigma ^2 = 1$.

\item La tercera, normalize$\_$datagen$\_$test, serÃ¡ igual que la anterior, pero no aÃ±adirÃ¡ el conjunto de validaciÃ³n, pues la utilizaremos para el conjunto de test, en el cual no queremos esta particiÃ³n

\item La cuarta, data$\_$augmentation$\_$datagen, harÃ¡ aumento de los datos haciendo inversiones horizontales de las imÃ¡genes y haciendo zoom.

\item La quinta, data$\_$augmentation$\_$datagen2, tratarÃ¡ hacer aumento de datos tambiÃ©n pero esta vez, harÃ¡ un relleno del borde (reflejÃ¡ndolo), tratarÃ¡ de girar las imÃ¡genes y hacer recortes sobre ellas

\item El Ãºltomo modelo,data$\_$augmentation$\_$all, unirÃ¡ las dos anteriores para futuras pruebas que haremos

\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{simple\PYZus{}datagen}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}\PY{n}{validation\PYZus{}split} \PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{)}
  \PY{k}{return} \PY{n}{datagen}

\PY{k}{def} \PY{n+nf}{normalize\PYZus{}datagen}\PY{p}{(}\PY{p}{)}\PY{p}{:}  
  \PY{n}{datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}
    \PY{n}{featurewise\PYZus{}center}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}              \PY{c+c1}{\PYZsh{} set mean = 0}
    \PY{n}{featurewise\PYZus{}std\PYZus{}normalization}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}   \PY{c+c1}{\PYZsh{} divide data by deviation         }
    \PY{n}{validation\PYZus{}split} \PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{)}
  \PY{k}{return} \PY{n}{datagen}

\PY{k}{def} \PY{n+nf}{normalize\PYZus{}datagen\PYZus{}test}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}
    \PY{n}{featurewise\PYZus{}center}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}              \PY{c+c1}{\PYZsh{} set mean = 0}
    \PY{n}{featurewise\PYZus{}std\PYZus{}normalization}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}   \PY{c+c1}{\PYZsh{} divide data by deviation         }
  \PY{p}{)}
  \PY{k}{return} \PY{n}{datagen}

\PY{k}{def} \PY{n+nf}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}
    \PY{n}{featurewise\PYZus{}center}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}              \PY{c+c1}{\PYZsh{} set mean = 0}
    \PY{n}{featurewise\PYZus{}std\PYZus{}normalization}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}   \PY{c+c1}{\PYZsh{} divide data by deviation            }
    \PY{n}{horizontal\PYZus{}flip}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}                 \PY{c+c1}{\PYZsh{} randomly horizontal flip images}
    \PY{n}{zoom\PYZus{}range} \PY{o}{=} \PY{l+m+mf}{0.2}\PY{p}{,}
    \PY{n}{validation\PYZus{}split} \PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{)}
  \PY{k}{return} \PY{n}{datagen}

\PY{k}{def} \PY{n+nf}{data\PYZus{}augmentation\PYZus{}datagen2}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}
    \PY{n}{featurewise\PYZus{}center}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}              \PY{c+c1}{\PYZsh{} set mean = 0}
    \PY{n}{featurewise\PYZus{}std\PYZus{}normalization}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}   \PY{c+c1}{\PYZsh{} divide data by deviation            }
    \PY{n}{fill\PYZus{}mode} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reflect}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,}               \PY{c+c1}{\PYZsh{} Reflect borders}
    \PY{n}{rotation\PYZus{}range} \PY{o}{=} \PY{l+m+mi}{45}\PY{p}{,}                  \PY{c+c1}{\PYZsh{} Rotate images 45 degrees}
    \PY{n}{shear\PYZus{}range} \PY{o}{=} \PY{l+m+mf}{0.2}\PY{p}{,}                     \PY{c+c1}{\PYZsh{} Deformate images in axis}
    \PY{n}{validation\PYZus{}split} \PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{)}
  \PY{k}{return} \PY{n}{datagen}  

\PY{k}{def} \PY{n+nf}{data\PYZus{}augmentation\PYZus{}all}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}
   \PY{n}{featurewise\PYZus{}center}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}              \PY{c+c1}{\PYZsh{} set mean = 0}
   \PY{n}{featurewise\PYZus{}std\PYZus{}normalization}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}   \PY{c+c1}{\PYZsh{} divide data by deviation            }
   \PY{n}{horizontal\PYZus{}flip}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}                 \PY{c+c1}{\PYZsh{} randomly horizontal flip images}
   \PY{n}{zoom\PYZus{}range} \PY{o}{=} \PY{l+m+mf}{0.2}\PY{p}{,}                     \PY{c+c1}{\PYZsh{} randomly zoom 0.2}
   \PY{n}{fill\PYZus{}mode} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reflect}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,}               \PY{c+c1}{\PYZsh{} Reflect borders}
   \PY{n}{rotation\PYZus{}range} \PY{o}{=} \PY{l+m+mi}{45}\PY{p}{,}                  \PY{c+c1}{\PYZsh{} Rotate images 45 degrees}
   \PY{n}{shear\PYZus{}range} \PY{o}{=} \PY{l+m+mf}{0.2}\PY{p}{,}                    \PY{c+c1}{\PYZsh{} Deformate images in axis}
   \PY{n}{validation\PYZus{}split} \PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{)}
  \PY{k}{return} \PY{n}{datagen}  
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{el-modelo-basenet}{%
\section{El modelo BaseNet}\label{el-modelo-basenet}}

Lo primero que haremos es comentar quÃ© \textbf{tipos de capas}
utilizaremos durante la definciÃ³n de los modelos:

\begin{itemize}
\item
  Capas de \emph{ConvoluciÃ³n (Conv2d)}. Estas realizarÃ¡n convoluciones
  sobre la entrada. En concreto, le daremos como parÃ¡metros un
  \emph{kernel\_size}, y el nÃºmero de filtros diferentes que harÃ¡n estas
  capas. La salida que ofrecerÃ¡n dependerÃ¡ del tamaÃ±o del \emph{kernel}
  que le pasemos y del nÃºmero de filtros que le indiquemos que aplique.
\item
  Capas de activaciÃ³n \emph{Relu}, que aplicarÃ¡n a cada elemento \(x_i\)
  (en nuestro caso, al principio serÃ¡n pÃ­xeles) de la entrada \(X\) la
  funciÃ³n \[
  g(x_i) = max(0,x_i)
  \] Estas nos aportarÃ¡n la no-linealidad al modelo que entrenaremos.
\item
  Capas de \emph{Pooling}. En concreto, utilizaremos
  \emph{MaxPooling}(aunque existen otras variantes como
  \emph{MinPooling, AveragePooling}) casi siempre. Esta recibirÃ¡ un
  parÃ¡metro que serÃ¡ el tamaÃ±o que queremos para obtener subvectores de
  nuestro vector (multi o unidimensional) de entrada, y tomarÃ¡ el mÃ¡ximo
  de ese vector como salida por cada subvector.
\item
  Capas \emph{Flatten}, que harÃ¡n que nuestra matriz pase a ser un
  vector, aÃ±adiendo cada fila de la misma al final de la anterior,
  empezando por la primera.
\item
  Capas \emph{Linear} (\emph{Dense} en \emph{Keras}), que aplicarÃ¡n a la
  entrada un producto por un vector que tendrÃ¡ pesos que se mejorarÃ¡n
  durante el entrenamiento.
\item
  ActivaciÃ³n \emph{Softmax}, que obtendrÃ¡ un vector que tendrÃ¡ tantas
  entradas nÃºmero de clases \(K\) tengamos para clasificar, y tendrÃ¡
  como output en la posiciÃ³n \emph{i-}Ã©sima la probabilidad de la imagen
  de pertenecer a la clase \emph{i-}Ã©sima. UtilizarÃ¡ como distribuciÃ³n
  de probabilidad la exponencial, estÃ¡ definido de la siguiente forma:
  \[ 
  {\displaystyle \sigma (\mathbf {z} )_{i}={\frac {e^{z_{i}}}{\sum _{j=1}^{K}e^{z_{j}}}}{\text{ for }}i=1,\dotsc ,K{\text{ and }}\mathbf {z} =(z_{1},\dotsc ,z_{K})\in \mathbb {R} ^{K}}
  \]
\end{itemize}

Comenzamos definiendo el modelo mÃ¡s sencillo pedido, \textbf{BaseNet}.

\begin{table}[H]
\begin{tabular}{|c|c|c|c|c|}
\hline
Layer No. & Layer Type   & \begin{tabular}[c]{@{}c@{}}Kernel size\\ conv layers\end{tabular} & \begin{tabular}[c]{@{}c@{}}Input | Output\\ dimension\end{tabular} & \begin{tabular}[c]{@{}c@{}}Input | Output\\ channels\\ conv layers\end{tabular} \\ \hline
1         & Conv2D       & 5                                                                 & 32 | 28                                                            & 3 | 6                                                                           \\ \hline
2         & Relu         & -                                                                 & 28 | 28                                                            & -                                                                               \\ \hline
3         & MaxPooling2D & 2                                                                 & 28 | 14                                                            & -                                                                               \\ \hline
4         & Conv2D       & 5                                                                 & 14 | 10                                                            & 6 | 16                                                                          \\ \hline
5         & Relu         & -                                                                 & 10 | 10                                                            & -                                                                               \\ \hline
6         & MaxPooling2D & 2                                                                 & 10 | 5                                                             & -                                                                               \\ \hline
7         & Linear       & -                                                                 & 400 | 50                                                           & -                                                                               \\ \hline
8         & Relu         & -                                                                 & 50 | 50                                                            & -                                                                               \\ \hline
9         & Linear       & -                                                                 & 50 | 25                                                            & -                                                                               \\ \hline
\end{tabular}
\end{table}
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{BaseNet}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}Layer 1 and 2}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}Layer 3}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}Layer 4 and 5}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}Layer 6}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}Flatten before FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}Layer 7 and 8}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}Layer 9 and softmax}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

  \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

  

Entrenaremos este modelo sobre una parte del conjunto de datos
\emph{Cifar100}, que contiene imÃ¡genes a color de tamaÃ±o
\(32 \times 32\) con 100 clases. Sin embargo, nosotros tomaremos una
parte de este conjunto de datos y reduciremos a 25 el nÃºmero de clases,
es por esto que la salida de la Ãºltima capa \emph{Linear} tiene que ser
de tamaÃ±o 25, para que podamos aplicarle una activaciÃ³n \emph{Softmax}.

Ahora, vamos a implementar una funciÃ³n que haga el entrenamiento y
muestre los resultados del mismo. Para ello, recibirÃ¡: 

\begin{itemize}
\item El
\emph{modelo} de red convolucional que va a utilizar para entrenar
\item El
tipo de generador de datos que va a usar para el conjunto de
entrenamiento, ya que hemos declarado dos funciones para que generen los
datos de forma mÃ¡s completa o menos. No tendrÃ¡ parÃ¡metro por defecto 
\item El tipo de generador de datos que se utilizarÃ¡ para el conjunto de test.
El parÃ¡metro mÃ¡s usado en este caso (se usarÃ¡ siempre salvo en la
primera prueba del modelo) serÃ¡ \emph{normalize\_datagen\_test}, un
objeto de la clase \emph{ImageDataGenerator} creado especÃ­ficamente para
este propÃ³sito. 
\item Un parÃ¡metro \emph{data\_loader} que se encargarÃ¡ de
llamar luego a la funciÃ³n que queramos para cargar los datos (nos
servirÃ¡ para usar esta misma funciÃ³n para la Ãºltima parte de la
prÃ¡ctica) 
\item Un booleano \emph{early\_stopping}, inicializado por defecto
a \emph{False}, que indicarÃ¡ si queremos hacer una parada anticipada en
nuestro modelo. 
\item El tamaÃ±o del \emph{batch}, que serÃ¡ por defecto
\emph{32} 
\item El nÃºmero de \emph{Ã©pocas}, que por defecto serÃ¡ \emph{50}.

\end{itemize}

Para compilar el modelo se nos pide un optimizador. Existen varios
optimizadores conocidos, pero utilizaremos en principio el
\textbf{Gradiente Descendiente EstocÃ¡stico} (SGD).

Esta funciÃ³n devolverÃ¡ el \emph{histograma} obtenido de cara a poder
hacer comparaciones futuras con otros modelos. El cÃ³digo es el
siguiente:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model}\PY{p}{,}\PY{n}{train\PYZus{}data\PYZus{}gen}\PY{p}{,}\PY{n}{test\PYZus{}data\PYZus{}gen} \PY{o}{=} \PY{n}{normalize\PYZus{}datagen\PYZus{}test}\PY{p}{,} \PY{n}{data\PYZus{}loader} \PY{o}{=} \PY{n}{loadImgs}\PY{p}{,}\PY{n}{early\PYZus{}stopping} \PY{o}{=} \PY{k+kc}{False}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{:}
  \PY{c+c1}{\PYZsh{} Declare optimizer : SGD}
  \PY{n}{opt} \PY{o}{=} \PY{n}{SGD}\PY{p}{(}\PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.01}\PY{p}{,}\PY{n}{decay} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{,}\PY{n}{momentum} \PY{o}{=} \PY{l+m+mf}{0.9}\PY{p}{,}\PY{n}{nesterov} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}Compile model:}
  \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}
        \PY{n}{loss} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{losses}\PY{o}{.}\PY{n}{categorical\PYZus{}crossentropy}\PY{p}{,}
        \PY{n}{optimizer} \PY{o}{=} \PY{n}{opt}\PY{p}{,}  
        \PY{n}{metrics} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Images}
  \PY{n}{x\PYZus{}train} \PY{p}{,} \PY{n}{y\PYZus{}train} \PY{p}{,} \PY{n}{x\PYZus{}test} \PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{data\PYZus{}loader}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Data generation object for train and test}
  \PY{n}{datagen} \PY{o}{=} \PY{n}{train\PYZus{}data\PYZus{}gen}\PY{p}{(}\PY{p}{)}
  \PY{n}{datagen\PYZus{}test} \PY{o}{=} \PY{n}{test\PYZus{}data\PYZus{}gen}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Data fit, in case we used featurewise\PYZus{}center and/or featurewise\PYZus{}std\PYZus{}normalization  in the dataGen}
  \PY{n}{datagen}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}
  \PY{n}{datagen\PYZus{}test}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Create train,test and validation sets}
  \PY{n}{train} \PY{o}{=} \PY{n}{datagen}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}
                     \PY{n}{y\PYZus{}train}\PY{p}{,}
                     \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{batch\PYZus{}size}\PY{p}{,} 
                     \PY{n}{subset} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{n}{validation} \PY{o}{=} \PY{n}{datagen}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}
                          \PY{n}{y\PYZus{}train}\PY{p}{,}
                          \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{batch\PYZus{}size}\PY{p}{,}
                          \PY{n}{subset} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
  \PY{n}{test} \PY{o}{=} \PY{n}{datagen\PYZus{}test}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{,}
                           \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}
                           \PY{n}{shuffle} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Check early stopping}
  \PY{n}{callbacks} \PY{o}{=} \PY{p}{[}\PY{p}{]}
  \PY{k}{if} \PY{n}{early\PYZus{}stopping}\PY{p}{:}
    \PY{n}{callbacks}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{EarlyStopping}\PY{p}{(}\PY{n}{monitor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}\PY{n}{restore\PYZus{}best\PYZus{}weights} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Generate histogram, fitting generator}
  \PY{n}{hist} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit\PYZus{}generator}\PY{p}{(}\PY{n}{generator} \PY{o}{=} \PY{n}{train}\PY{p}{,} 
                            \PY{n}{steps\PYZus{}per\PYZus{}epoch} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.9}\PY{o}{/}\PY{n}{batch\PYZus{}size}\PY{p}{,}
                            \PY{n}{epochs} \PY{o}{=} \PY{n}{epochs}\PY{p}{,} 
                            \PY{n}{validation\PYZus{}data} \PY{o}{=} \PY{n}{validation}\PY{p}{,}
                            \PY{n}{validation\PYZus{}steps} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.1}\PY{o}{/}\PY{n}{batch\PYZus{}size}\PY{p}{,}
                            \PY{n}{verbose} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,}
                            \PY{n}{callbacks} \PY{o}{=} \PY{n}{callbacks}
                            \PY{p}{)}
  \PY{c+c1}{\PYZsh{} Calculate predictoins}
  \PY{n}{pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}generator}\PY{p}{(}\PY{n}{test}\PY{p}{,}
                                 \PY{n}{steps} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test}\PY{p}{)}\PY{p}{)}
  \PY{n}{score} \PY{o}{=} \PY{n}{calculateAccuracy}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{pred}\PY{p}{)}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TEST SCORE =  }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{score}\PY{p}{)}\PY{p}{)}
  \PY{k}{return} \PY{n}{hist}
\end{Verbatim}
\end{tcolorbox}

    Pasamos a probar un entrenamiento de \textbf{BaseNet}. En este primer
caso, el objeto de la clase \emph{ImageDataGenerator} que necesitamos,
se crearÃ¡ Ãºnicamente con un parÃ¡metro: - \emph{validation\_split = 0.1},
que nos indica que el tamaÃ±o de la particiÃ³n de validaciÃ³n serÃ¡ del
\(10\%\)

Se utilizarÃ¡ este \emph{datagen} tanto para el conjunto de \emph{train}
como para el conjunto de \emph{test}.

Vemos la ejecuciÃ³n del entrenamiento y los resultados:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model1} \PY{o}{=} \PY{n}{BaseNet}\PY{p}{(}\PY{p}{)}
\PY{n}{hist} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model1}\PY{p}{,}\PY{n}{simple\PYZus{}datagen}\PY{p}{,}\PY{n}{simple\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.394
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_13_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_13_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Como podemos ver a partir de unas 10 Ã©pocas el valor de la funciÃ³n de
pÃ©rdida comienza a subir, mientras que el valor de acierto del modelo se
ha establecido en el intervalo \((0.37,0.4)\) desde aproximadamente unas
7 Ã©pocas, segÃºn la ejecuciÃ³n. Esto nos estÃ¡ indicando que el modelo ha
aprendido lo suficiente y, como vemos, comienza a hacer
\emph{overfitting} en los datos, pues hay una gran diferencia entre el
acierto en el conjunto de train y en el conjunto de validaciÃ³n cuando
hay un aumento de Ã©pocas.

Recordemos que en cada ejecuciÃ³n los resultados pueden variar, pues los
valores de los pesos se inicializan aleatoriamente cada vez.

\hypertarget{mejora-del-modelo}{%
\section{Mejora del modelo}\label{mejora-del-modelo}}

Una vez que hemos implementado este modelo, vamos a pasar a realizar las
mejoras sobre este. Se proponen una serie de mejoras que iremos tratando
una por una y estudiando cÃ³mo varÃ­a el modelo. Comenzamos

\hypertarget{normalizaciuxf3n-de-los-datos}{%
\subsection{NormalizaciÃ³n de los
datos}\label{normalizaciuxf3n-de-los-datos}}

Para la primera mejora, vamos a normalizar los datos para que tengan
\(\mu = 0\) y \(\sigma^2 = 1\). Para ello tenemos la funciÃ³n que
habÃ­amos declarado anteriormente \textbf{\emph{normalize\_datagen}}, que
el objeto de la clase \emph{ImageDataGenerator} genere los datos como
nos interesa. En concreto, tendrÃ¡ dos nuevos parÃ¡metros, que serÃ¡n: -
\emph{featurewise\_center=True}, que nos harÃ¡ que los datos tengan
\(\mu = 0\) - \emph{featurewise\_std\_normalization=True} que harÃ¡ que
los datos tengan \(\sigma^2 = 1\).

Con esto, solo tenemos que crear un nuevo modelo \emph{BaseNet}, pero
llamando esta vez en la funciÃ³n de ejecuciÃ³n a a la funciÃ³n ya
mencionada. Veamos el resultado:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model2} \PY{o}{=} \PY{n}{BaseNet}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}model2.summary()}
\PY{n}{hist2} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model2}\PY{p}{,}\PY{n}{normalize\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.3788
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_15_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Podemos observar que, en caso de que haya mejora (debemos recordad que
en este proceso influye la aleatoriedad, pues los pesos iniciales son
aleatorios), la mejora que se obtiene es muy leve en el porcentaje de
acierto del modelo cuando se ha utilizado este nuevo \emph{datagen} que
normaliza los datos.

Sin embargo, en modelos sucesivos, vamos a seguir utilizando la
normalizaciÃ³n del conjunto de datos pues es posible que al aÃ±adir mÃ¡s
capas a nuestro modelo pueda sernos de utilidad para estas capas.

\hypertarget{aumento-de-los-datos}{%
\subsection{Aumento de los datos}\label{aumento-de-los-datos}}

Una buena tÃ©cnica para mejorar el entrenamiento de nuestra red es
realizar \textbf{data augmentation}. Este consiste en modificar de
diversas maneras las imÃ¡genes que tenemos de entrenamiento de forma que
puedan obtener diferentes caracterÃ­sticas y puedan aportar por ello mÃ¡s
informaciÃ³n a nuestro modelo. Algunas de las acciones mÃ¡s interesantes
que podemos hacer para realizar este aumento de datos son: -
\emph{Vertical/Horizontal Flip}, que voltearÃ¡ una imagen en el sentido
que le podamos indicar - \emph{Zoom}, que harÃ¡ zoom sobre una imagen
segÃºn un rango que le indiquemos - \emph{Rotation}, que harÃ¡ una
rotaciÃ³n de nuestra imagen en el sentido indicado.

Hay que mencionar que este aumento de datos no se realiza siempre, sino
que existe una probabilidad de que se le aplique a una imagen de forma
aleatoria, de forma que no se realizarÃ¡ el aumento de datos que podamos
indicar a todas las imÃ¡genes que tengamos para el entrenamiento.

Ahora, vamos a realizar el mismo procedimiento que en el caso anterior,
pero vamos a aÃ±adir que el generador de datos haga ademÃ¡s un
\textbf{data\_augmentation}. Comenzamos por uno sencillo. Para ello,
ademÃ¡s de los parÃ¡metros anteriores (que tambiÃ©n estarÃ¡n en nuestro
objeto \emph{datagen}, de cara a hacer comparativas entre los resultados
obtenidos), se crearÃ¡ el objeto con los siguientes parÃ¡metros: -
\emph{horizontal\_flip = True}, que harÃ¡ que algunas imÃ¡genes hagan
\emph{flip} aleatoriamente - \emph{zoom\_range = 0.2}, que harÃ¡ que se
haga zoom aleatoriamente sobre algunas de las imÃ¡genes.

Veamos ahora los resultados de la ejecuciÃ³n.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model3} \PY{o}{=} \PY{n}{BaseNet}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}model3.summary()}
\PY{n}{hist3} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model3}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{25}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.4528
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_17_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Volvemos a obtener en este caso una mejora de \(0.02\), lo cual es
bastante significativo , pues obtener una mejora del \(0.01\) por ciento
sabemos que ya es complicado.

Sobre todo ,podemos observar en los histogramas que la precisiÃ³n de
acierto asciende mÃ¡s y se hace mÃ¡s cercana al \emph{Training Accuracy}
que en los casos anteriores, que es lo que nos interesa a la hora de
clasificar. Este es un buen criterio para afirmar que el modelo es
\textbf{mejor} cuando se normalizan los datos.

Ahora, serÃ­a interesante probar otros tipos de
\emph{data\_augmentation}. Como se ha podido comprobar al principio en
las funciones, temnemos otra llamada \emph{data\_augmentation\_datagen2}
que harÃ¡ otro aumento de los datos diferente. En concreto, los
parÃ¡metros que tiene adicionales a la normalizaciÃ³n son:

\begin{itemize} 
\item \emph{fill\_mode = `reflect'}, que se encarga de hacer que al aplicar
los filtros, el borde que se utilice sea \emph{BORDER\_REFLECT}
\item \emph{rotation\_range = 45}, que harÃ¡ que las imÃ¡genes puedan rotar un
Ã¡ngulo de 45 grados. 
\item  \emph{shear\_range = 0.2}, que harÃ¡ que se puedan
hacer ciertos cortes en la imagen.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model\PYZus{}augmentation2} \PY{o}{=} \PY{n}{BaseNet}\PY{p}{(}\PY{p}{)}
\PY{n}{hist\PYZus{}augmentation2} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model\PYZus{}augmentation2}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen2}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{25}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist\PYZus{}augmentation2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.4068
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_19_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_19_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Parece que estas grÃ¡ficas se ajustan algo mÃ¡s a las del conjunto de training, aunque el resultado final de acierto en el conjunto de test sea menor. Sin embargo, tambiÃ©n vemos que a partir de la Ã©poca 20, la funciÃ³n de pÃ©rdida comienza a aumentar, al contrario de en el caso anterior.

Ahora, vamos a hacer una prueba utilizando todos los parÃ¡metros que
hemos usado anteriormente , pero a la vez. Veamos si tener una gran
cantidad de formas de hacer \emph{data\_augmentation} nos proporciona un
mejor resultado. Utilizamos la funciÃ³n \emph{data\_augmentation\_all}
que hemos definido.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model\PYZus{}augmentation\PYZus{}all} \PY{o}{=} \PY{n}{BaseNet}\PY{p}{(}\PY{p}{)}
\PY{n}{hist\PYZus{}augmentation\PYZus{}all} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model\PYZus{}augmentation\PYZus{}all}\PY{p}{,}\PY{n}{train\PYZus{}data\PYZus{}gen} \PY{o}{=} \PY{n}{data\PYZus{}augmentation\PYZus{}all}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{25}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist\PYZus{}augmentation\PYZus{}all}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.386
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_21_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_21_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    El resultado es peor que los anteriores. Vemos que las grÃ¡ficas de validaciÃ³n se ajustan a las grÃ¡ficas de entrenamiento, pero aun asÃ­, las grÃ¡ficas de entrenamiento estÃ¡n obteniendo valores por debajo de los que hemos conseguido anteriormente, por  lo que parece que necesitarÃ­a mÃ¡s tiempo en Ã©pocas para entrenar los pesos.

Vamos ahora a hacer la comparaciÃ³n entre los 3, para ver quÃ© hemos
obtenido:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{compareHists}\PY{p}{(}\PY{p}{[}\PY{n}{hist3}\PY{p}{,}\PY{n}{hist\PYZus{}augmentation2}\PY{p}{,}\PY{n}{hist\PYZus{}augmentation\PYZus{}all}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Flip and zoom}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fill, rotation and shear}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{All data augmentation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.7\linewidth}{0.9\paperheight}}{output_23_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.7\linewidth}{0.9\paperheight}}{output_23_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Podemos ver que, al comparar los 3, obtenemos que el mejor
\emph{accuracy} en la validaciÃ³n lo obtenemos en el modelo de datos que
hace solamente \emph{Flip} y \emph{Zoom} de las imÃ¡genes. Por debajo
queda el que hace \emph{Fill} y \emph{Rotation}, y el Ãºltimo queda el
que hace todas a la vez. Esto podrÃ­a deberse a que hacer tantas
modificaciones sobre las imÃ¡genes ralentice el aprendizaje del modelo
pues le serÃ¡ mÃ¡s difÃ­cil reconocer caracterÃ­sticas si las puede
encontrar de diversas maneras.

AdemÃ¡s, el modelo que usa mÃ¡s formas de \emph{data augmentation} tiene
tambiÃ©n la funciÃ³n de pÃ©rdida mÃ¡s alta durante todo el tiempo, siendo el
que utiliza \emph{Flip and zoom} el que tiene la menor funciÃ³n de
pÃ©rdida.

Es por esto que, a partir de ahora, consideraremos el primer
\emph{data\_augmentation} como el mejor de los generadores de imÃ¡genes y
es el que utilizaremos de ahora en adelante.

    \hypertarget{red-muxe1s-profunda}{%
\subsection{Red mÃ¡s profunda}\label{red-muxe1s-profunda}}

Vamos a tratar ahora de profundizar la red, aÃ±adiendo mÃ¡s capas a
nuestro modelo inicial.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{deeper\PYZus{}basenet}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} First conv +  relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Second Conv + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Third conv, bigger output size + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} 4th conv, 3x3 this time}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Flatten before FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC. add Relu output size = 100}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC and relu, smaller output size = 50}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC and softmax}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

  \PY{k}{return} \PY{n}{model}
  
\end{Verbatim}
\end{tcolorbox}

    Una vez definida, vemos que hemos aÃ±adido las siguientes capas: 
    
    \begin{itemize}
    \item \emph{Conv2D} despuÃ©s de las anteriores, pero con un \emph{outputSize}
mayor. 
\item \emph{Conv2D} antes de hacer \emph{Flatten}, pero con un tamaÃ±o
de kÃ©rnel menor al de las anteriores 
\item \emph{Dense}, una nueva capa FC
pero con un tamaÃ±o de salida mayor
\end{itemize}

Con esto, tratamos de ver si aÃ±adiendo mÃ¡s capas de convoluciÃ³n con
parÃ¡metros parecidos a los que ya tenÃ­amos anteriormente, obtenemos una
mejora en los resultados.

A partir de ahora, utilizaremos el objeto de \emph{ImageDataGenerator}
que nos da mejores resultados hasta el momento, es decir, el que realiza
\textbf{dataAugmentation}.

Vamos ahora a ver los resultados. Ejecutamos dos veces, uno con 15
Ã©pocas y otro con 40, para ver cÃ³mo varÃ­a segÃºn las Ã©pocas. Las dos
primeras imÃ¡genes se corresponderÃ¡n con las \(15\) Ã©pocas y las dos
segundas con las \(40\) Ã©pocas.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model4} \PY{o}{=} \PY{n}{deeper\PYZus{}basenet}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}model4.summary()}
\PY{n}{hist4} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model4}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist4}\PY{p}{)}
\PY{n}{hist5} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model4}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{45}\PY{p}{)}

\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.4012
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.4\paperheight}}{output_28_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.4\paperheight}}{output_28_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.4224
Epochs = 40
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_28_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_28_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Podemos observar como hay un empeoramiento de los resultados respecto a
nuestro mejor modelo (que conseguÃ­a \(0.45\) de acierto en \emph{test}),
a pesar incluso de establecer un nÃºmero de Ã©pocas bastante mayor. Esto
nos indica que aÃ±adir mÃ¡s capas no tiene por quÃ© mejorar los resultados.
Tampoco aunque aumentemos el nÃºmero de Ã©pocas esto tiene por quÃ©
mejorar.

AdemÃ¡s, podemos ver que estamos tratando de buscar casi el doble de
parÃ¡metros que en los anteriores modelos.

    \hypertarget{uso-de-kernels-muxe1s-pequeuxf1os}{%
\subsubsection{Uso de Kernels mÃ¡s
pequeÃ±os}\label{uso-de-kernels-muxe1s-pequeuxf1os}}

Vamos a probar ahora a tomar tamaÃ±os de kernel siempre de \(3\times 3\),
y veamos cÃ³mo afecta esto a los resultados. Llamaremos al nuevo modelo
\textbf{small\_kernel\_basenet}, y serÃ¡ igual que el anterior salvo que:
- Todas las \emph{Conv2D} tendrÃ¡n \(kernel\_size = (3,3)\) - Quitaremos
la capa FC que tenÃ­a como \emph{OutputSize = 50}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{small\PYZus{}kernel\PYZus{}basenet}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} First conv +  relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Second Conv + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Third conv, bigger output size + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} 4th conv, 3x3 this time}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Flatten before FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC. add Relu output size = 100}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC and softmax}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

  \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model5} \PY{o}{=} \PY{n}{small\PYZus{}kernel\PYZus{}basenet}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}model5.summary()}
\PY{n}{hist6} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model5}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist6}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.478
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_32_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_32_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    En este caso, se consigue una pequeÃ±a mejora, vemos como ambas grÃ¡ficas
se acercan mÃ¡s a las grÃ¡ficas de \emph{entrenamiento}, lo cual quiere
decir que los pesos que estÃ¡ obteniendo nuestra red neuronal en el
entrenamiento son suficientemente buenos como para obtener un acierto
parecido en el entrenamiento y en la validaciÃ³n despuÃ©s.

Esto quiere decir que nuestro modelo estÃ¡ entrenando lo suficiente (pues
vemos que el acierto en validaciÃ³n llega a un punto en el que no
asciende con notoriedad, lo cual nos indica que hemos aprendido mÃ¡s o
menos lo que podÃ­amos en general), y que , como nos estamos acercando
con el acierto en validaciÃ³n al acierto en \emph{train}, estamos
aprendiendo buenas caracterÃ­sticas que nos ayudan a obtener el mayor
porcentaje de acierto posible en este caso.

\hypertarget{batch-normalization}{%
\subsection{Batch Normalization}\label{batch-normalization}}

\textbf{Batch Normalization} es una tÃ©cnica para mejorar el
funcionamiento, velocidad y estabilidad de nuestra red neuronal
convolucional. Consiste en normalizar segÃºn \emph{batches}, es decir,
tomar un \emph{batch} de imÃ¡genes que se tomarÃ¡ del tamaÃ±o
\emph{batch\_size} que hayamos utilizado, y hacer una normalizaciÃ³n
usando solo esas imÃ¡genes.

Vamos a tratar de aÃ±adir a nuestro modelo algunas capas de
\emph{BatchNormalization} para reducir el \emph{overfitting} de nuestro
modelo. Lo haremos sobre el Ãºltimo modelo que hemos utilizado, que es el que
parece que nos ha dado los mejores resultados aunque tenga un nÃºmero de
parÃ¡metros a entrenar bastante mayor que el resto de modelos.

Vamos a aÃ±adir estas capas de \emph{BatchNormalization} antes y despuÃ©s
de las capas convolucionales, y compararemos los resultados obtenidos.

\hypertarget{antes-de-relu}{%
\subsubsection{\texorpdfstring{Antes de
\emph{relu}}{Antes de relu}}\label{antes-de-relu}}

Vamos a definir primero un modelo en el que el \emph{Batch
Normalization} se harÃ¡ despuÃ©s de las activaciones \emph{relu}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{bn\PYZus{}after\PYZus{}relu\PYZus{}basenet}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} First conv +  relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Second Conv + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Another Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Third conv, bigger output size + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} 4th conv, 3x3 this time}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Flatten before FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC. add Relu output size = 100}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC and softmax}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

  \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model6} \PY{o}{=} \PY{n}{bn\PYZus{}after\PYZus{}relu\PYZus{}basenet}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}model6.summary()}
\PY{n}{hist7} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model6}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist7}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.5064
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_35_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_35_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    El cambio es bastante significativo. Podemos ver en la grÃ¡fica que en
estas 30 Ã©pocas, la funciÃ³n de \emph{score} no para de aumentar ni si
quiera en el conjunto de entrenamiento. Se obtiene una clasificaciÃ³n que
es mejor en mÃ¡s 10 puntos porcentuales con respecto al modelo inicial,
lo cual es un cambio sigficativo, llegando casi a quedarse en \(0.5\) de
aciertos en el conjunto de test.

Podemos tambiÃ©n apreciar que nos aparecen \emph{44 non trainable
parameters} (parÃ¡metros no entrenables), esto nos indica que tenemos
parÃ¡metros que no se pueden optimizar con los datos que tenemos. Estos
probablemente hayan aparecido de haber introducido capas como
\emph{Batch Normalization} o \emph{Relu}, que son capas que no se
entrenan.

Este es hasta ahora el mejor modelo obtenido , gracias a su buen
\emph{test\_score} y a que su proximidad de las grÃ¡ficas de validaciÃ³n y
las de train es mayor que en el resto de casos.

\hypertarget{despuuxe9s-de-relu}{%
\subsubsection{DespuÃ©s de Relu}\label{despuuxe9s-de-relu}}

Ahora, vamos a ver cÃ³mo se comporta el modelo si hacemos esta
\emph{Batch Normalization} despuÃ©s de hacer la activaciÃ³n \emph{relu}. A
priori, el resultado podrÃ­a variar, pues al hacer normalizaciones en los
batch, podrÃ­a haber alguna modificaciÃ³n en los valores negativos de este
y cambiar algunos valores obtenidos en la aplicaciÃ³n de \emph{relu}.

Definamos el modelo que tenga ahora las activaciones \emph{relu} despuÃ©s
de las normalizaciones en Batch:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{bn\PYZus{}before\PYZus{}relu\PYZus{}basenet}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} First conv +  relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Now ReLu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Second Conv + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Another Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{}Relu after Batch}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Activation}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Third conv, bigger output size + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} 4th conv, 3x3 this time}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Flatten before FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC. add Relu output size = 100}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC and softmax}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

  \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model7} \PY{o}{=} \PY{n}{bn\PYZus{}before\PYZus{}relu\PYZus{}basenet}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}model7.summary()}
\PY{n}{hist8} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model7}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist8}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.5056
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_38_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_38_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Vamos a comparar ambas grÃ¡ficas y ver quÃ© resultados podemos obtener:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{compareHists}\PY{p}{(}\PY{p}{[}\PY{n}{hist7}\PY{p}{,}\PY{n}{hist8}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BatchNormalization after relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Batch normalization before relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_40_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_40_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Podemos observar como el resultado que se obtiene al evaluar en el
conjunto de test no difiere mucho en ambos modelos. Sin embargo, si
estudiamos las grÃ¡ficas, vemos que:
\begin{itemize} 
\item La funciÃ³n de pÃ©rdida es
prÃ¡cticamente siempre inferior cuando \emph{Batch Normalization} se
realiza \textbf{despuÃ©s} de \emph{relu}. 
\item  El acierto es siempre mayor
en los conjuntos de validaciÃ³n durante el entrenamiento cuando
\emph{Batch Normalization} se realiza \textbf{despuÃ©s} de \emph{relu}.
\end{itemize}

A pesar de no haber mucha diferencia, hemos obtenido que si hacemos la
normalizaciÃ³n \textbf{despuÃ©s} de hacer la activaciÃ³n \emph{relu}, tanto
la funciÃ³n de pÃ©rdida es un poco menor que haciÃ©ndo la normalizaciÃ³n
antes, como el acierto es algo mayor, asÃ­ que podrÃ­amos decir que
obtenemos mejores resultados si hacemos \emph{Batch Normalization}
\textbf{despuÃ©s} de hacer \emph{relu}. Es por ello que a partir de ahora
tomaremos en los sucesivos modelos como base el modelo
\emph{bn\_after\_relu\_basenet}

\hypertarget{batch-normalization-en-capas-fc}{%
\subsubsection{Batch Normalization en capas
FC}\label{batch-normalization-en-capas-fc}}

Vamos a hacer ahora una nueva prueba, trataremos de hacer \emph{Batch
Normalization} , ademÃ¡s de despuÃ©s de algunas \emph{relu}, despuÃ©s de
alguna capa \emph{Dense} y veamos si esto mejora sustancialmente o
empeora los resultados.

Definamos primero el modelo,partiendo del modelo que ya hemos
mencionado.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{bn\PYZus{}fc\PYZus{}basenet}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} First conv +  relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Second Conv + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Another Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Third conv, bigger output size + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} 4th conv, 3x3 this time}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Flatten before FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC. add Relu output size = 100}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Batch normalization in FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC and softmax}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

  \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

    Ahora, entrenamos este modelo y lo comparamos directamente con el
resultado del mismo modelo pero sin aÃ±adir la normalizaciÃ³n en la capa
FC.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model8} \PY{o}{=} \PY{n}{bn\PYZus{}fc\PYZus{}basenet}\PY{p}{(}\PY{p}{)}
\PY{n}{hist9} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model8}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist9}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.52
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_44_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_44_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Los comparamos en la misma grÃ¡fica

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{compareHists}\PY{p}{(}\PY{p}{[}\PY{n}{hist7}\PY{p}{,}\PY{n}{hist9}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Batch Normalization not in FC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Batch Normalization in FC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_46_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_46_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Los resultados nos muestran lo siguiente:

\begin{itemize}
\tightlist
\item
  Aunque en el comienzo sea mayor, la funciÃ³n de pÃ©rdida es generalmente
  \textbf{menor} cuando tenemos \emph{Batch Normalization} en las capas
  FC que cuando no la tenemos.
\item
  \emph{Batch Normalization} en las capas FC nos proporciona un mayor
  acierto en el conjunto de validaciÃ³n
\item
  El \emph{test score} final es muy parecido entre ambas. En la Ãºltima
  ejecuciÃ³n hecha, el mayor ha sido en la que tiene \emph{BN} en la capa
  \emph{FC}, por lo que de ahora en adelante usaremos este modelo como
  base.
\end{itemize}

Con estos resultados, podemos enunciar que hacer \emph{Batch
Normalization} en las Ãºltimas capas nos estÃ¡ ayudando a obtener un
entrenamiento mÃ¡s general que no utilizarlo en este tipo de capas, por
lo cual, utilizarlo en ellas supone una \textbf{mejora} respecto a
nuestro modelo.

\hypertarget{early-stopping}{%
\subsection{Early Stopping}\label{early-stopping}}

Una buena opciÃ³n en nuestro entrenamiento puede ser hacer una parada del
entrenamiento si el modelo no consigue mejorar tras pasar un nÃºmero de
Ã©pocas que nos interese, para que no se alargue en tiempo de ejecuciÃ³n
innecesariamente. Esto es justamente el \emph{early stopping}.

Vamos a utilizar ahora el parÃ¡metro que hemos definido en nuestra
funciÃ³n de entrenamiento. \emph{Early Stopping} nos ayudarÃ¡ a que el
entrenamiento termine de forma anticipada y evitarÃ¡ asÃ­ el posible
\emph{overfitting}. AdemÃ¡s, al terminar de forma anticipada el
entrenamiento, habremos conseguido mÃ¡s rapidez en el mismo.

Comentar primero que un \textbf{callback} es , segÃºn la documentaciÃ³n de
\emph{Keras}, un conjunto de funciones que se aplican en ciertos
momentos del proceso de entrenamiento. Una de las funciones que se le
puede aplicar es \emph{Early Stopping}.

Si le indicamos a nuestra funciÃ³n que queremos \emph{Early Stopping}, se
le indicarÃ¡ a \emph{fit\_generator} que el conjunto de \emph{callbacks}
estÃ© formado por la funciÃ³n \emph{EarlyStopping}. Esta funciÃ³n tiene
varios parÃ¡metros, pero los que tendremos en cuenta serÃ¡n:
\begin{itemize} \item
\emph{monitor}, que es el parÃ¡metro que queremos ver si estÃ¡ cambiando
durante las Ã©pocas 

\item \emph{patience}, que es el nÃºmero de Ã©pocas que
queremos esperar sin que se modifique para parar el entrenamiento
\end{itemize}
En nuestro caso, creo que esto no serÃ¡ muy relevante pues el nÃºmero de
Ã©pocas que estamos usando en el entrenamiento es muy pequeÃ±o (recordemos
que es solamente 30). AsÃ­ que, para que pudiera tener algo de impacto en
el entrenamiento, vamos a establecer que nuestro algoritmo tenga ``poca
paciencia'', y haga \emph{EarlyStopping} si no mejora en \textbf{3}
Ã©pocas.

Vamos a ver cÃ³mo actÃºa el mejor modelo obtenido hasta ahora si activamos
\emph{Early Stopping}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model8} \PY{o}{=} \PY{n}{bn\PYZus{}fc\PYZus{}basenet}\PY{p}{(}\PY{p}{)}
\PY{n}{hist9} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model8}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{early\PYZus{}stopping}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist9}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.5244
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_48_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_48_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Vemos como hemos obtenido un \emph{early stopping} a las 10 Ã©pocas,
porque no hemos conseguido entre la Ã©poca 9 y 10 que la funciÃ³n de
pÃ©rdida disminuya en el conjunto de validaciÃ³n, como querÃ­amos ver. A
costa de esto, hemos obtenido una pequeÃ±a pÃ©rdida en el \emph{test
score} final, descendiendo en dos puntos porcentuales.

Es posible que el nÃºmero de Ã©pocas de \emph{patience} que estamos
estableciendo no sea suficiente. Sin embargo,

    \hypertarget{la-verdadera-mejora-dropout}{%
\section{La verdadera mejora :
Dropout}\label{la-verdadera-mejora-dropout}}

Tras este proceso de pruebas y mejoras, vamos a utilizar ahora la capa
que nos va a dar un salto mayor en el acierto en el conjunto de test. Se
trata de las capas \textbf{Dropout}.

Las capas \emph{Dropout} harÃ¡n que las neuronas de nuestro modelo tengan
una probabilidad de dejar de ser entrenadas. En concreto, dada una
probabilidad \emph{rate} que pasaremos como parÃ¡metro,lo que harÃ¡ segÃºn
la documentaciÃ³n de \emph{keras} es poner los inputs para esa neurona a
0, lo cual nos ayudarÃ¡ a prevenir el overfitting.

Dejar de entrenar ciertas neuronas prevendrÃ¡ la creaciÃ³n de
inter-dependencias entre las neuronas, lo que nos ayudarÃ¡ a obtener un
modelo mÃ¡s robusto en cuanto a dependencias entre nodos.

Vamos a definir un modelo basÃ¡ndonos en nuestro mejor modelo anterior.
Haibing y Xiaodong en {[}@Haibing-Xiaodong{]} mencionan que aunque al
principio se utilizaban solo \emph{dropouts} en capas que no estuvieran
\emph{FC}, finalmente se encontrÃ³ que usarlos tambiÃ©n en capas \emph{FC}
reduce el error en el \emph{test}, asÃ­ que en nuestro modelo lo
utilizaremos en ambas partes.

{[}@Haibing-Xiaodong{]}: https://arxiv.org/pdf/1512.00242.pdf ``Towards
Dropout Training for Convolutional Neural Networks''

Pasamos a definir el modelo. Vemos primero la tabla, y luego el cÃ³digo.

\begin{table}[H]
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Layer No. & Layer Type         & \begin{tabular}[c]{@{}c@{}}Kernel size\\ conv layers\end{tabular} & \begin{tabular}[c]{@{}c@{}}Input | Output\\ dimension\end{tabular} & \begin{tabular}[c]{@{}c@{}}Input | Output\\ channels\\ conv layers\end{tabular} & \begin{tabular}[c]{@{}c@{}}Dropout rate\\ dropout layers\end{tabular} \\ \hline
1         & Conv2D             & 3                                                                 & 30                                                                 & 3 | 6                                                                           & -                                                                     \\ \hline
2         & Relu               & -                                                                 & 30 | 30                                                            & -                                                                               & -                                                                     \\ \hline
3         & BatchNormalization & -                                                                 & 30 | 30                                                            & -                                                                               & -                                                                     \\ \hline
4         & MaxPooling2D       & 2                                                                 & 30 | 15                                                            & -                                                                               & -                                                                     \\ \hline
5         & Dropout            & -                                                                 & 15 | 15                                                            & -                                                                               & 0.25                                                                  \\ \hline
6         & Conv2D             & 3                                                                 & 15 | 13                                                            & 6 | 16                                                                          & -                                                                     \\ \hline
7         & Relu               & -                                                                 & 13 | 13                                                            & -                                                                               & -                                                                     \\ \hline
8         & BatchNormalization & -                                                                 & 13 | 13                                                            & -                                                                               & -                                                                     \\ \hline
9         & Conv2D             & 3                                                                 & 13 | 11                                                            & 16 | 32                                                                         & -                                                                     \\ \hline
10        & MaxPooling2D       & 2                                                                 & 11 | 5                                                             & -                                                                               & -                                                                     \\ \hline
11        & Dropout            & -                                                                 & 5 | 5                                                              & -                                                                               & 0.25                                                                  \\ \hline
12        & Flatten            & -                                                                 & 5 | 800                                                            & -                                                                               & -                                                                     \\ \hline
13        & Linear             & -                                                                 & 800 | 100                                                          & -                                                                               & -                                                                     \\ \hline
14        & Relu               & -                                                                 & 100 | 100                                                          & -                                                                               & -                                                                     \\ \hline
15        & BatchNormalization & -                                                                 & 100 | 100                                                          & -                                                                               & -                                                                     \\ \hline
16        & Dropout            & -                                                                 & 100 | 100                                                          & -                                                                               & 0.5                                                                   \\ \hline
17        & Linear             & -                                                                 & 100 | 25                                                           & -                                                                               & -                                                                     \\ \hline
\end{tabular}
\end{table}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{dropout\PYZus{}basenet}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} First conv +  relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Dropout}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.25}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Second Conv + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Another Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Third conv, bigger output size + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Dropout}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.25}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} 4th conv, 3x3 this time}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Flatten before FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC. add Relu output size = 100}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Batch normalization in FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Dropout }
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC and softmax}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

  \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

    Hemos aÃ±adido: - \emph{Dropout} con \(rate = 0.25\) despuÃ©s del primer
\emph{MaxPooling} - \emph{Dropout} con \(rate = 0.25\) despuÃ©s del
segundo (y Ãºltimo) \emph{MaxPooling) - }Dropout* con \(rate = 0.5\)
antes de la Ãºltima capa \emph{Dense+Softmax}

Veamos el comportamiento del mismo.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model9} \PY{o}{=} \PY{n}{dropout\PYZus{}basenet}\PY{p}{(}\PY{p}{)}
\PY{n}{hist10} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model9}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{50}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.4936
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_53_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_53_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Sorprendentemente, tenemos que la grÃ¡fica de validaciÃ³n estÃ¡ por debajo
en la funciÃ³n de pÃ©rdida. Esto es una noticia bastante buena, aunque
difÃ­cil de comprender. Significa que nuestro modelo estÃ¡ generalizando
mejor incluso de lo que puede conseguir solo con el \emph{train
dataset}.

Se podrÃ­a pensar que los ratios de \emph{Dropout} que se estÃ¡n dando son
demasiado altos. Sin embargo, un ratio mÃ¡s pequeÃ±o harÃ­a que
prÃ¡cticamente no se notara respecto a otras ejecuciones que no tuvieran
\emph{Dropout}. Vamos a hacer la prueba, cambiaremos el \emph{rate} de
los tres anteriores por \(0.1\) en el prÃ³ximo modelo, lo ejecutaremos y
luego los compararemos.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{low\PYZus{}dropout\PYZus{}basenet}\PY{p}{(}\PY{p}{)}\PY{p}{:}
  \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} First conv +  relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Dropout}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Second Conv + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Another Batch Normalization}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Third conv, bigger output size + relu}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Reduce size}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Dropout}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} 4th conv, 3x3 this time}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,}\PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Flatten before FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC. add Relu output size = 100}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Batch normalization in FC}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Dropout }
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} FC and softmax}
  \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

  \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model10} \PY{o}{=} \PY{n}{low\PYZus{}dropout\PYZus{}basenet}\PY{p}{(}\PY{p}{)}
\PY{n}{hist11} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model10}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{50}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist11}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.58
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_56_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_56_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Y, con esto, acabamos de obtener el mejor \emph{test\_score} hasta
ahora. Vemos como, en comparaciÃ³n con la anterior ejecuciÃ³n con
\emph{Dropout}, las grÃ¡ficas de validaciÃ³n quedan ahora (como es de
esperar), con peores resultados que las grÃ¡ficas de \emph{training}.

Sin embargo, es un muy buen resultado pues vemos que el \emph{accuracy}
de validaciÃ³n queda muy cerca de la grÃ¡fica del \emph{accuracy} de
\emph{training}.

Veamos la comparaciÃ³n entre ambas grÃ¡ficas por ver quiÃ©n podrÃ­amos decir
que es ``mejor''.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{compareHists}\PY{p}{(}\PY{p}{[}\PY{n}{hist10}\PY{p}{,}\PY{n}{hist11}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{High Dropout}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Low Dropout}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_58_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{output_58_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Se ve claramente como , teniendo un \emph{Dropout} bajo, tenemos una
grÃ¡fica de pÃ©rdida \textbf{mayor} (lo cual es peor), y una grÃ¡fica de
\textbf{accuracy} mayor, por lo que podrÃ­amos decir que \emph{en
general}, es mejor utilizar unos niveles mÃ¡s altos de \emph{Dropout}.


    
    
 \hypertarget{transferencia-de-modelos-y-fine-tuning}{%
\section{Transferencia de Modelos y Fine
Tuning}\label{transferencia-de-modelos-y-fine-tuning}}

En esta secciÃ³n, trataremos de utilizar modelos ya existentes sobre un
problema concreto que se nos ha proporcionado: la clasificaciÃ³n de tipos
de pÃ¡jaros. Utilizaremos el modelos previamente entrenados y veremos
cÃ³mo de bien funciona este modelo sobre nuestro problema.

A partir de ahora utilizaremos una base de datos diferente. SerÃ¡ la base
de datos \emph{Caltech-UCSD}. Este conjunto tiene 200 clases diferentes
con 3000 imÃ¡genes en el \emph{train set} y 3033 en el \emph{test set}.
Dejaremos de nuevo un \(10\%\) del \emph{train set} para la validaciÃ³n.

\hypertarget{transferencia-de-modelos}{%
\subsection{Transferencia de modelos}\label{transferencia-de-modelos}}

Vamos a usar como extractor de caracterÃ­sticas el modelo
\textbf{ResNet50}, preentrenado en \emph{ImageNet}. Especificaremos la
opciÃ³n \emph{pooling = `avg'}, para que se nos devuelva de la funciÃ³n el
modelo con una capa \emph{GlobalAveragePooling} al final, teniendo
entonces la salida en una dimensiÃ³n.

Con el parÃ¡metro \emph{include\_top = False}, estamos quitando la Ãºltima
capa que clasificaba las imÃ¡genes, asÃ­ que nos servirÃ¡ para extraer las
caracterÃ­sticas, que es lo que queremos. A la funciÃ³n que nos crearÃ¡ el
modelo A la funciÃ³n que nos crea el modelo, se le pasarÃ¡ un parÃ¡metro
\emph{freeze} , que nos servirÃ¡ para \emph{fine-tuning} y serÃ¡ explicado
mÃ¡s adelante.

Tendremos como salida un vector de 2048 caracterÃ­sticas, con el que
podremos entrenar otro modelo. Para comenzar, lo usaremos como extractor
de caracterÃ­sticas y aÃ±adiremos Ãºnicamente una capa \emph{Dense} para
comprobar si las caracterÃ­sticas extraidas son buenas para nuestro
modelo o no.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{res\PYZus{}net\PYZus{}50}\PY{p}{(}\PY{n}{freeze} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,}\PY{n}{num\PYZus{}freeze} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
  \PY{n}{resnet50} \PY{o}{=} \PY{n}{ResNet50}\PY{p}{(}\PY{n}{weights}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imagenet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                      \PY{n}{include\PYZus{}top}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
                      \PY{n}{pooling}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{avg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                      \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,}\PY{l+m+mi}{224}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Freeze layers for fine\PYZhy{}tuning}
  \PY{k}{if} \PY{n}{freeze}\PY{p}{:}
    \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n}{resnet50}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}freeze}\PY{p}{]}\PY{p}{:}
      \PY{k}{if} \PY{p}{(}\PY{o+ow}{not} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n}{layer}\PY{p}{,} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{BatchNormalization}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{layer}\PY{o}{.}\PY{n}{trainable} \PY{o}{=} \PY{k+kc}{False}
      
  \PY{c+c1}{\PYZsh{} new model to add softmax}
  \PY{n}{x} \PY{o}{=} \PY{n}{resnet50}\PY{o}{.}\PY{n}{output}
  \PY{c+c1}{\PYZsh{} Adding softmax}
  \PY{n}{x} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{x}\PY{p}{)}
  \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{n}{inputs} \PY{o}{=} \PY{n}{resnet50}\PY{o}{.}\PY{n}{input}\PY{p}{,} \PY{n}{outputs} \PY{o}{=} \PY{n}{x}\PY{p}{)}
  \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{res\PYZus{}net\PYZus{}50}\PY{p}{(}\PY{n}{freeze} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}

\PY{n}{hist} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{data\PYZus{}loader} \PY{o}{=} \PY{n}{load\PYZus{}caltech\PYZus{}data}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

TEST SCORE =  0.3425651170458292
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_9_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_9_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Podemos observar varias cosas de estos resultados

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  El modelo entrena muy bien en el conjunto de \emph{train}. Esto es
  fÃ¡cilmente observable en que la funciÃ³n \emph{loss} tiende a \(0\)
  solamente con 10 Ã©pocas. AdemÃ¡s, tambiÃ©n llega casi a un \(100\%\) de
  acierto en este mismo conjunto.
\item
  Sin embargo, podemos ver que estas dos funciones tienen valores
  bastante malos en el conjunto de validaciÃ³n, comparados con los que
  hemos visto que toma en el conjunto de \emph{train}.
\item
  El \emph{score} que obtenemos al final (entorno a \(0.35\)), se
  asemeja bastante a los valores que se obtienen durante las Ã©pocas en
  el conjunto de validaciÃ³n. Sin embargo, se aleja bastante (al igual
  que los de validaciÃ³n), del acierto en el conjunto de \emph{train}.
\end{enumerate}

Con estos tres puntos, podemos decir que el modelo estÃ¡ aprendiendo
bastante pero solamente del conjunto de entrenamiento y no es un buen
aprendizaje para la base de datos en general.

\hypertarget{fine--tuning}{%
\subsection{Fine- Tuning}\label{fine--tuning}}

En el uso prÃ¡ctico de \emph{CNNs}, los modelos suelen tener una cantidad
inmensa de parÃ¡metros que es complicado entrenar. Es por ello que se
suele utilizar \textbf{\emph{fine-tuning}} sobre estos modelos para no
tener que entrenarlos de forma completa y partir ya de un entrenamiento
previo de estos. Esto se puede hacer de forma general porque el
entrenamiento que suelen tener ha sido hecho en \emph{datasets} de
tamaÃ±o mucho mayor que el que nosotros utilizaremos normalmente (como en
este caso, que el nÃºmero de imÃ¡genes que tenemos es bastante reducido,
poco mÃ¡s de 6K), y por tanto el modelo habrÃ¡ aprendido caracterÃ­sticas
que serÃ¡n potencialmente buenas para nuestros datos.

Una prÃ¡ctica bastante recomendada al hacer \emph{fine-tuning} es la de
hacer \textbf{freeze}, que consiste en congelar determinadas capas de
nuestro modelo para que , si vuelves a entrenar el modelo de forma
completa, estas capas no hagan el entrenamiento.

Esto podrÃ­a hacerse para mantener las primeras capas, que suelen
preocuparse de recoger caracterÃ­sticas mÃ¡s generales (curvas,bordes),
intactas pues queremos mantener ese conocimiento en nuestro modelo. Es
por ello que pasaremos \emph{freeze = true} cuando queramos hacer
\emph{fine-tuning} de nuestra red, y esto harÃ¡ que todas las capas del
modelo inicial se congelen.

Hay que tener cuidado pues parece que hay un \textbf{bug}, y
\emph{keras} no congela bien las capas que son de
\emph{BatchNormalization}. Es por ello por lo que hemos aÃ±adido un
\emph{if} en nuestro cÃ³digo que comprueba si es o no una capa de
\emph{BatchNormalization}, y si lo es, no congela esa capa. De todos
modos, estas capas son \textbf{no-entrenables}, asÃ­ que no supone un
problema para el entrenamiento de nuestro modelo.

Vamos ahora a probar a hacer \emph{fine-tuning} con la red completa, por
lo que serÃ¡ suficiente enviar el parÃ¡metro \emph{freeze = False}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model2} \PY{o}{=} \PY{n}{res\PYZus{}net\PYZus{}50}\PY{p}{(}\PY{n}{freeze} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}

\PY{n}{hist2} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model2}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{data\PYZus{}loader} \PY{o}{=} \PY{n}{load\PYZus{}caltech\PYZus{}data}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.36993076162215627
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_11_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_11_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Las grÃ¡ficas de validaciÃ³n parecen en este caso estar mÃ¡s alejadas del
\emph{training} que en el caso anterior. Sin embargo, tambiÃ©n parecen un
poco menos estabilizadas que las anteriores, por lo que con el paso de
las Ã©pocas podrÃ­an ir acerÃ¡ndose mÃ¡s a los buenos resultados tambiÃ©n en
la validaciÃ³n. Vamos a ver una comparaciÃ³n entre las funciones de
pÃ©rdida y de \emph{accuracy} en el conjunto de validaciÃ³n para los dos
tipos de modelos.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{compareHists}\PY{p}{(}\PY{p}{[}\PY{n}{hist}\PY{p}{,}\PY{n}{hist2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Freezed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not Freezed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_13_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Podemos ver como, al no congelar el modelo: - La funciÃ³n depÃ©rdida es
menor - El \emph{accuracy} en la validaciÃ³n acaba siendo mayor aunque
tarde un poco en superar al modelo que sÃ­ estÃ¡ congelado.

Esto podrÃ­a indicar que al fin y al cabo el modelo que no congela
acabarÃ¡ ajustando los datos mejor que el modelo que utiliza como
extractor de caracterÃ­sticas a \emph{ResNet50}.

\hypertarget{auxf1adiendo-capas-a-nuestro-extractor-de-caracteruxedsticas}{%
\subsection{AÃ±adiendo capas a nuestro extractor de
caracterÃ­sticas}\label{auxf1adiendo-capas-a-nuestro-extractor-de-caracteruxedsticas}}

Vamos a probar a aÃ±adir algunas capas mÃ¡s a parte de Ãºnicamente el
\emph{Softmax} y \emph{Dense} a Resnet, para ver si conseguimos mejorar
algo su resultado.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{0}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{res\PYZus{}net\PYZus{}50\PYZus{}plus}\PY{p}{(}\PY{n}{freeze} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,}\PY{n}{num\PYZus{}freeze} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
  \PY{n}{resnet50} \PY{o}{=} \PY{n}{ResNet50}\PY{p}{(}\PY{n}{weights}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imagenet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                      \PY{n}{include\PYZus{}top}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
                      \PY{n}{pooling}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{avg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                      \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,}\PY{l+m+mi}{224}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
  \PY{c+c1}{\PYZsh{} Freeze layers for fine\PYZhy{}tuning}
  \PY{k}{if} \PY{n}{freeze}\PY{p}{:}
    \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n}{resnet50}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{p}{:}\PY{n}{num\PYZus{}freeze}\PY{p}{]}\PY{p}{:}
      \PY{k}{if} \PY{p}{(}\PY{o+ow}{not} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n}{layer}\PY{p}{,} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{BatchNormalization}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{layer}\PY{o}{.}\PY{n}{trainable} \PY{o}{=} \PY{k+kc}{False}
      
  \PY{c+c1}{\PYZsh{} new model to add softmax}
  \PY{n}{x} \PY{o}{=} \PY{n}{resnet50}\PY{o}{.}\PY{n}{output}
  \PY{c+c1}{\PYZsh{} Adding softmax}
  \PY{n}{x} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1024}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{x}\PY{p}{)}
  \PY{n}{x} \PY{o}{=} \PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{(}\PY{n}{x}\PY{p}{)}
  \PY{n}{x} \PY{o}{=} \PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{(}\PY{n}{x}\PY{p}{)}
  \PY{n}{x} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{512}\PY{p}{)}\PY{p}{(}\PY{n}{x}\PY{p}{)}
  \PY{n}{x} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{x}\PY{p}{)}
  \PY{n}{x} \PY{o}{=} \PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,}\PY{n}{activation} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{(}\PY{n}{x}\PY{p}{)}
  
  \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{n}{inputs} \PY{o}{=} \PY{n}{resnet50}\PY{o}{.}\PY{n}{input}\PY{p}{,} \PY{n}{outputs} \PY{o}{=} \PY{n}{x}\PY{p}{)}
  \PY{k}{return} \PY{n}{model}
\end{Verbatim}
\end{tcolorbox}

    Hemos aÃ±adido despuÃ©s de las capas de \emph{ResNet50} las siguientes
capas para tratar de realizar una disminuciÃ³n progresiva de tamaÃ±o: -
Una capa \emph{Dense(1024)}, para reducir el tamaÃ±o a la mitad, seguida
por una activaciÃ³n \emph{Relu} - Un \emph{Dropout} de \(rate = 0.5\),
para tratar de eliminar un gran nÃºmero de parÃ¡metros - Otra capa
\emph{Dense(512)}, reduciendo de nuevo el tamaÃ±o - Una Ãºltima capa
\emph{Dense(256)} antes de la capa que tiene la activaciÃ³n
\emph{Softmax}

Veamos si da buenos resultados:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model\PYZus{}plus} \PY{o}{=} \PY{n}{res\PYZus{}net\PYZus{}50\PYZus{}plus}\PY{p}{(}\PY{n}{freeze} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,}\PY{n}{num\PYZus{}freeze} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{hist\PYZus{}plus} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model\PYZus{}plus}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist\PYZus{}plus}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

      \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.14968677876689745
    \end{Verbatim}
    
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_17_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Los resultados en cuanto a \emph{test score} son estrepitosos. El modelo no es capaz de alcanzar ni un
acierto de \(0.15\) en el conjunto de test, aunque su funciÃ³n de pÃ©rdida
disminuya de forma tan drÃ¡stica, vemos que el acierto en la validaciÃ³n
no aumenta tanto como el acierto en \emph{training}, que, por otro lado,
aumenta de forma mucho mÃ¡s lenta que en los casos anteriores.

Descartaremos estos cambios al no haber sido una mejora sustancial del
modelo.

    \hypertarget{usando-fine-tuning-en-parte-de-la-red}{%
\subsection{Usando Fine-Tuning en parte de la
red}\label{usando-fine-tuning-en-parte-de-la-red}}

Por Ãºltimo, vamos a hacer una pequeÃ±a prueba. Como he comentado antes,
en fine-tuning puede decidirse congelar una serie de capas, y no todas.
En concreto, podrÃ­a ser interesante congelar todas las primeras capas y
dejar las Z Ãºltimas sin congelar, para tratar de usar las primeras como
extractores de ``caracterÃ­sticas bÃ¡sicas'' de las imÃ¡genes , y entrenar
las Ãºltimas para usarlas en nuestra base de datos concreta.

Es para ello para lo que le hemos aÃ±adido a nuestro modelo un parÃ¡metro
con el cual podremos indicar cuÃ¡ntos layers (empezando por el final)
querremos no congelar. Vamos a hacer una prueba con Z=10, y veremos el
resultado:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model3} \PY{o}{=} \PY{n}{res\PYZus{}net\PYZus{}50}\PY{p}{(}\PY{n}{freeze} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,}\PY{n}{num\PYZus{}freeze} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{hist\PYZus{}freeze10} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}show}\PY{p}{(}\PY{n}{model3}\PY{p}{,}\PY{n}{data\PYZus{}augmentation\PYZus{}datagen}\PY{p}{,}\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{showEvolution}\PY{p}{(}\PY{n}{hist\PYZus{}freeze10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

      \begin{Verbatim}[commandchars=\\\{\}]
TEST SCORE =  0.4038905374216947
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_20_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_20_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Podemos comprobar como, en este caso, se estabilizan mÃ¡s pronto las
funciones tanto de validaciÃ³n como de pÃ©rdida

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{compareHists}\PY{p}{(}\PY{p}{[}\PY{n}{hist\PYZus{}freeze10}\PY{p}{,}\PY{n}{hist2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Freeze 10}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Freeze all}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_22_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.5\linewidth}{0.9\paperheight}}{part2/output_22_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    AdemÃ¡s, en esta comparaciÃ³n, vemos que si congelamos 10 capas la pÃ©rdida es menor y el acierto es mayor. Esto podrÃ­a indicarnos que el modelo estÃ¡ extrayendo buenas caracterÃ­sticas y entrenar las Ãºltimas capas podrÃ­a estar ayudÃ¡ndonos incluso habiendo sido obtenido con los pesos de otra base de datos diferente.
    
    \hypertarget{conclusiones}{%
\section{Conclusiones}\label{conclusiones}}

Durante la prÃ¡ctica, hemos tratado de empezar con un modelo bÃ¡sico de
red neuronal convolucional , \textbf{BaseNet} y hemos ido tratando de
mejorarla poco a poco, hasta llegar a un modelo que tiene un porcentaje
de acierto en el conjunto de \emph{test} bastante decente, \(0.58\).

Hemos descubierto que muchas veces, aÃ±adir mÃ¡s Ã©pocas o mÃ¡s capas a los
modelos pueden llevarnos a \emph{overfitting} y por tanto no nos
interesa introducir capas aleatoriamente ni poner un nÃºmero de Ã©pocas
muy elevado.

AdemÃ¡s, hemos encontrado que utilizar \emph{Dropouts} mejora
sustancialmente los resultados en cuanto a menor diferencia entre las
grÃ¡ficas de la funciÃ³n de pÃ©rdida del conjunto de validaciÃ³n y el de
entrenamiento, asÃ­ como de la funciÃ³n de acierto, por lo que es una de
las mejores mejoras que se le pueden aÃ±adir a nuestro modelo, aunque hay
que tener cuidado de establecer un \(rate\) adecuado.

Por Ãºltimo, hemos tratado de usar una red ya entrenada ,
\textbf{ResNet50}, con los pesos de otro conjunto de Datos para ver cÃ³mo
se comporta utilizÃ¡ndola en una base de datos propia. Hemos obtenido
unos resultados que son, en general, bastante malos, no superando el
\(40\%\) de acierto incluso tratando de hacer diversas modificaciones y
mejoras, como aÃ±adir capas, o congelar un nÃºmero determinado de capas
mediante \emph{fine-tuning}.

Aunque no estÃ© considerado en una secciÃ³n como tal, es posible que pudieran ser considerados como parte del bonus la mejora con \emph{Dropout} y la mejora con \emph{fine-tuning} de no todas las capas.

\hypertarget{bibliografuxeda}{%
\section{BibliografÃ­a}\label{bibliografuxeda}}

https://marubon-ds.blogspot.com/2017/08/how-to-make-fine-tuning-model.html

https://keras.io/preprocessing/image/

https://en.m.wikipedia.org/wiki/Softmax\_function

https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/

https://flyyufelix.github.io/2016/10/03/fine-tuning-in-keras-part1.html

https://arxiv.org/pdf/1512.00242.pdf


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
